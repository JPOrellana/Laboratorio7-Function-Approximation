{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c88fe4a",
   "metadata": {},
   "source": [
    "\n",
    "### Laboratorio 7 — Monte Carlo con Aproximación de Funciones (CartPole-v1)\n",
    "\n",
    "Este notebook implementa **tres agentes** de Aprendizaje por Refuerzo con **Aproximación de Funciones** para el entorno **CartPole-v1** de Gymnasium, utilizando **Monte Carlo (MC)** como mecanismo de actualización:\n",
    "\n",
    "1. **Árbol de Decisión** (regresión)  \n",
    "2. **Random Forest** (ensamble de árboles)  \n",
    "3. **Red Neuronal Feed-Forward** (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4118c",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90285b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x177670e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e6da",
   "metadata": {},
   "source": [
    "### Utilidades y Representación de Características\n",
    "\n",
    "Representamos \\(Q(s,a)\\) con un modelo de regresión que recibe como entrada el estado continuo (escalado) y la acción codificada one-hot, y devuelve el valor estimado \\(Q(s,a)\\).\n",
    "\n",
    "Para árboles y random forest usamos scikit-learn. Para la red, usamos un MLP en PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1835654",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureCoder:\n",
    "    \"\"\"Escala estados y codifica acciones como one-hot, luego concatena [state_scaled || action_onehot].\"\"\"\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.state_scaler = StandardScaler()\n",
    "        self.action_enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "        self.n_actions = n_actions\n",
    "        self._fitted = False\n",
    "\n",
    "    def fit(self, states: np.ndarray, actions: np.ndarray):\n",
    "        self.state_scaler.fit(states)\n",
    "        self.action_enc.fit(actions.reshape(-1, 1))\n",
    "        self._fitted = True\n",
    "\n",
    "    def transform(self, states: np.ndarray, actions: np.ndarray) -> np.ndarray:\n",
    "        assert self._fitted, \"FeatureCoder no está ajustado. Llama primero a fit.\"\n",
    "        S = self.state_scaler.transform(states)\n",
    "        A = self.action_enc.transform(actions.reshape(-1, 1))\n",
    "        return np.hstack([S, A])\n",
    "\n",
    "    def fit_transform(self, states: np.ndarray, actions: np.ndarray) -> np.ndarray:\n",
    "        self.fit(states, actions)\n",
    "        return self.transform(states, actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334d0e",
   "metadata": {},
   "source": [
    "### Generación de Episodios y Retornos (Every-Visit MC)\n",
    "\n",
    "- Generamos episodios con política \\(\\epsilon\\)-greedy.  \n",
    "- Calculamos el retorno desde cada tiempo: \\(G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}\\).  \n",
    "- Every-visit: entrenamos con todas las visitas de \\((s_t, a_t)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b283228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class EpisodeStep:\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "\n",
    "def epsilon_greedy(q_values: np.ndarray, epsilon: float) -> int:\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(len(q_values))\n",
    "    return int(np.argmax(q_values))\n",
    "\n",
    "def compute_returns(rewards: List[float], gamma: float) -> List[float]:\n",
    "    G = 0.0\n",
    "    returns = [0.0] * len(rewards)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "        returns[t] = G\n",
    "    return returns\n",
    "\n",
    "def rollout_episode(env, q_predict_fn, epsilon: float, gamma: float, max_steps: int = 500) -> List[EpisodeStep]:\n",
    "    state, _ = env.reset(seed=SEED)\n",
    "    steps: List[EpisodeStep] = []\n",
    "    for t in range(max_steps):\n",
    "        # Calcular Q(s, ·) con el aproximador\n",
    "        q_s = q_predict_fn(state)  \n",
    "        action = epsilon_greedy(q_s, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        steps.append(EpisodeStep(state=state, action=action, reward=reward))\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30837a64",
   "metadata": {},
   "source": [
    "### Interfaz de Agente con Aproximador \\(Q(s,a)\\)\n",
    "\n",
    "Creamos una clase base para manejar:\n",
    "- Predicción de \\(Q(s,a)\\) en lote para todas las acciones.\n",
    "- Buffer de experiencias MC \\((s,a,G)\\).\n",
    "- Entrenamiento por bloques (refit periódico para modelos de sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb66d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseQFunction:\n",
    "    def __init__(self, n_actions: int, state_dim: int):\n",
    "        self.n_actions = n_actions\n",
    "        self.state_dim = state_dim\n",
    "        self.feature_coder = FeatureCoder(n_actions=n_actions)\n",
    "        self._is_fitted = False\n",
    "        # replay/MC buffer\n",
    "        self._S = []\n",
    "        self._A = []\n",
    "        self._G = []\n",
    "\n",
    "    def add_mc_batch(self, states: np.ndarray, actions: np.ndarray, returns: np.ndarray):\n",
    "        self._S.append(states)\n",
    "        self._A.append(actions)\n",
    "        self._G.append(returns)\n",
    "\n",
    "    def _stack_buffer(self):\n",
    "        if not self._S:\n",
    "            return None, None\n",
    "        S = np.vstack(self._S)\n",
    "        A = np.concatenate(self._A).astype(int)\n",
    "        G = np.concatenate(self._G).astype(float)\n",
    "        return S, A, G\n",
    "\n",
    "    def predict_q(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Devuelve Q(s, ·) como vector de tamaño n_actions.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_from_buffer(self):\n",
    "        \"\"\"Entrena/refit a partir de todo el buffer acumulado.\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427de6a",
   "metadata": {},
   "source": [
    "\n",
    "### Agente con Árbol de Decisión (Regresión)\n",
    "\n",
    "- Modelo: DecisionTreeRegressor\n",
    "- Entrenamiento: refit sobre todo el buffer acumulado cada cierto número de episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68514be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTreeQ(BaseQFunction):\n",
    "    def __init__(self, n_actions: int, state_dim: int, **tree_kwargs):\n",
    "        super().__init__(n_actions, state_dim)\n",
    "        self.model = DecisionTreeRegressor(**tree_kwargs)\n",
    "\n",
    "    def predict_q(self, state: np.ndarray) -> np.ndarray:\n",
    "        if not self._is_fitted:\n",
    "            return np.zeros(self.n_actions, dtype=float)\n",
    "        states = np.tile(state, (self.n_actions, 1))\n",
    "        actions = np.arange(self.n_actions)\n",
    "        X = self.feature_coder.transform(states, actions)\n",
    "        preds = self.model.predict(X)\n",
    "        return preds.astype(float)\n",
    "\n",
    "    def train_from_buffer(self):\n",
    "        data = self._stack_buffer()\n",
    "        if data[0] is None:\n",
    "            return\n",
    "        S, A, G = data\n",
    "        if not self._is_fitted:\n",
    "            X = self.feature_coder.fit_transform(S, A)\n",
    "        else:\n",
    "            X = self.feature_coder.transform(S, A)\n",
    "        y = G\n",
    "        self.model.fit(X, y)\n",
    "        self._is_fitted = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767f4e3",
   "metadata": {},
   "source": [
    "\n",
    "### Agente con Random Forest\n",
    "\n",
    "- Modelo: RandomForestRegressor  \n",
    "- Entrenamiento: refit sobre buffer acumulado (mejor estabilidad que un árbol único)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be87ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomForestQ(BaseQFunction):\n",
    "    def __init__(self, n_actions: int, state_dim: int, **rf_kwargs):\n",
    "        super().__init__(n_actions, state_dim)\n",
    "        self.model = RandomForestRegressor(**rf_kwargs)\n",
    "\n",
    "    def predict_q(self, state: np.ndarray) -> np.ndarray:\n",
    "        if not self._is_fitted:\n",
    "            return np.zeros(self.n_actions, dtype=float)\n",
    "        states = np.tile(state, (self.n_actions, 1))\n",
    "        actions = np.arange(self.n_actions)\n",
    "        X = self.feature_coder.transform(states, actions)\n",
    "        preds = self.model.predict(X)\n",
    "        return preds.astype(float)\n",
    "\n",
    "    def train_from_buffer(self):\n",
    "        data = self._stack_buffer()\n",
    "        if data[0] is None:\n",
    "            return\n",
    "        S, A, G = data\n",
    "        if not self._is_fitted:\n",
    "            X = self.feature_coder.fit_transform(S, A)\n",
    "        else:\n",
    "            X = self.feature_coder.transform(S, A)\n",
    "        y = G\n",
    "        self.model.fit(X, y)\n",
    "        self._is_fitted = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7643efb",
   "metadata": {},
   "source": [
    "\n",
    "### Agente con Red Neuronal (MLP)\n",
    "\n",
    "- Arquitectura simple (2–3 capas ocultas).  \n",
    "- Pérdida MSE contra los retornos MC como *targets*.  \n",
    "- Entrenamiento por minibatches a partir del buffer acumulado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473c2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: List[int] = [128, 128]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, 1))  # salida escalar Q(s,a)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NeuralNetQ(BaseQFunction):\n",
    "    def __init__(self, n_actions: int, state_dim: int, hidden=[128,128], lr=1e-3, device=None):\n",
    "        super().__init__(n_actions, state_dim)\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # in_dim = state_dim + n_actions (one-hot)\n",
    "        self.model = MLP(in_dim=state_dim + n_actions, hidden=hidden).to(self.device)\n",
    "        self.opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def predict_q(self, state: np.ndarray) -> np.ndarray:\n",
    "        if not self._is_fitted:\n",
    "            return np.zeros(self.n_actions, dtype=float)\n",
    "        states = np.tile(state, (self.n_actions, 1))\n",
    "        actions = np.arange(self.n_actions)\n",
    "        X = self.feature_coder.transform(states, actions)\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            q = self.model(X_t).squeeze(-1).cpu().numpy()\n",
    "        return q.astype(float)\n",
    "\n",
    "    def train_from_buffer(self, epochs: int = 5, batch_size: int = 256):\n",
    "        data = self._stack_buffer()\n",
    "        if data[0] is None:\n",
    "            return\n",
    "        S, A, G = data\n",
    "        if not self._is_fitted:\n",
    "            X = self.feature_coder.fit_transform(S, A)\n",
    "            self._is_fitted = True\n",
    "        else:\n",
    "            X = self.feature_coder.transform(S, A)\n",
    "        y = G.reshape(-1, 1)\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        y_t = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        ds = torch.utils.data.TensorDataset(X_t, y_t)\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        self.model.train()\n",
    "        for _ in range(epochs):\n",
    "            for xb, yb in dl:\n",
    "                pred = self.model(xb)\n",
    "                loss = self.loss_fn(pred, yb)\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090fd06",
   "metadata": {},
   "source": [
    "\n",
    "### Bucle de Entrenamiento MC y Evaluación\n",
    "\n",
    "- Entrenamiento: generamos episodios, computamos retornos y agregamos \\((s,a,G)\\) al buffer.  \n",
    "- Reajustamos el modelo cada update_freq episodios.  \n",
    "- Evaluación periódica (\\(\\epsilon=0\\)) para medir retorno promedio y tasa de éxito (episodios con 500 pasos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    episodes: int = 500\n",
    "    gamma: float = 0.99\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.05\n",
    "    epsilon_decay: int = 300  \n",
    "    update_freq: int = 5       \n",
    "    eval_every: int = 20\n",
    "    eval_episodes: int = 10\n",
    "    max_steps: int = 500\n",
    "\n",
    "def linear_decay(ep, start, end, decay):\n",
    "    frac = min(1.0, ep / decay)\n",
    "    return start + (end - start) * frac\n",
    "\n",
    "def evaluate_agent(env, q_predict_fn, cfg: TrainConfig) -> Tuple[float, float]:\n",
    "    returns = []\n",
    "    successes = 0\n",
    "    for _ in range(cfg.eval_episodes):\n",
    "        state, _ = env.reset(seed=np.random.randint(0, 10_000))\n",
    "        total_r = 0.0\n",
    "        for t in range(cfg.max_steps):\n",
    "            q_s = q_predict_fn(state)\n",
    "            action = int(np.argmax(q_s))  \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_r += reward\n",
    "            if terminated or truncated:\n",
    "                if t+1 >= cfg.max_steps:\n",
    "                    successes += 1\n",
    "                break\n",
    "        returns.append(total_r)\n",
    "    return float(np.mean(returns)), successes / cfg.eval_episodes\n",
    "\n",
    "def train_mc_agent(agent: BaseQFunction, cfg: TrainConfig, env_name: str = \"CartPole-v1\") -> Dict[str, List[float]]:\n",
    "    env = gym.make(env_name)\n",
    "    metrics = {\n",
    "        \"episode_return\": [],\n",
    "        \"eval_return\": [],\n",
    "        \"eval_success\": [],\n",
    "        \"epsilon\": []\n",
    "    }\n",
    "    for ep in range(1, cfg.episodes + 1):\n",
    "        epsilon = linear_decay(ep, cfg.epsilon_start, cfg.epsilon_end, cfg.epsilon_decay)\n",
    "        steps = rollout_episode(\n",
    "            env=env,\n",
    "            q_predict_fn=agent.predict_q,\n",
    "            epsilon=epsilon,\n",
    "            gamma=cfg.gamma,\n",
    "            max_steps=cfg.max_steps\n",
    "        )\n",
    "        rewards = [s.reward for s in steps]\n",
    "        Gs = compute_returns(rewards, cfg.gamma)\n",
    "\n",
    "        states = np.array([s.state for s in steps], dtype=np.float32)\n",
    "        actions = np.array([s.action for s in steps], dtype=int)\n",
    "        returns = np.array(Gs, dtype=np.float32)\n",
    "\n",
    "        agent.add_mc_batch(states, actions, returns)\n",
    "\n",
    "        if ep % cfg.update_freq == 0:\n",
    "            # entrenar/refit\n",
    "            if isinstance(agent, NeuralNetQ):\n",
    "                agent.train_from_buffer(epochs=5, batch_size=256)\n",
    "            else:\n",
    "                agent.train_from_buffer()\n",
    "\n",
    "        metrics[\"episode_return\"].append(sum(rewards))\n",
    "        metrics[\"epsilon\"].append(epsilon)\n",
    "\n",
    "        if ep % cfg.eval_every == 0:\n",
    "            avg_ret, succ = evaluate_agent(env, agent.predict_q, cfg)\n",
    "            metrics[\"eval_return\"].append(avg_ret)\n",
    "            metrics[\"eval_success\"].append(succ)\n",
    "            print(f\"[Ep {ep}] EvalReturn={avg_ret:.1f}  Success={succ*100:.0f}%  Eps={epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56518b",
   "metadata": {},
   "source": [
    "\n",
    "### Entrenar y Comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310814b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 20] EvalReturn=46.1  Success=0%  Eps=0.94\n",
      "[Ep 40] EvalReturn=22.1  Success=0%  Eps=0.87\n",
      "[Ep 60] EvalReturn=60.0  Success=0%  Eps=0.81\n",
      "[Ep 80] EvalReturn=42.5  Success=0%  Eps=0.75\n",
      "[Ep 100] EvalReturn=42.9  Success=0%  Eps=0.68\n",
      "[Ep 120] EvalReturn=111.1  Success=0%  Eps=0.62\n",
      "[Ep 140] EvalReturn=92.1  Success=0%  Eps=0.56\n",
      "[Ep 160] EvalReturn=96.2  Success=0%  Eps=0.49\n",
      "[Ep 180] EvalReturn=96.3  Success=0%  Eps=0.43\n",
      "[Ep 200] EvalReturn=71.0  Success=0%  Eps=0.37\n",
      "[Ep 220] EvalReturn=118.7  Success=0%  Eps=0.30\n",
      "[Ep 240] EvalReturn=116.5  Success=0%  Eps=0.24\n",
      "[Ep 260] EvalReturn=103.0  Success=0%  Eps=0.18\n",
      "[Ep 280] EvalReturn=90.9  Success=0%  Eps=0.11\n",
      "[Ep 300] EvalReturn=60.2  Success=0%  Eps=0.05\n",
      "[Ep 320] EvalReturn=80.4  Success=0%  Eps=0.05\n",
      "[Ep 340] EvalReturn=89.7  Success=0%  Eps=0.05\n",
      "[Ep 360] EvalReturn=57.7  Success=0%  Eps=0.05\n",
      "[Ep 380] EvalReturn=88.1  Success=0%  Eps=0.05\n",
      "[Ep 400] EvalReturn=59.2  Success=0%  Eps=0.05\n",
      "[Ep 20] EvalReturn=215.8  Success=0%  Eps=0.94\n",
      "[Ep 40] EvalReturn=175.1  Success=0%  Eps=0.87\n",
      "[Ep 60] EvalReturn=202.0  Success=0%  Eps=0.81\n",
      "[Ep 80] EvalReturn=122.8  Success=0%  Eps=0.75\n",
      "[Ep 100] EvalReturn=314.6  Success=20%  Eps=0.68\n",
      "[Ep 120] EvalReturn=226.8  Success=0%  Eps=0.62\n",
      "[Ep 140] EvalReturn=235.3  Success=0%  Eps=0.56\n",
      "[Ep 160] EvalReturn=213.8  Success=0%  Eps=0.49\n",
      "[Ep 180] EvalReturn=264.8  Success=10%  Eps=0.43\n",
      "[Ep 200] EvalReturn=182.1  Success=0%  Eps=0.37\n",
      "[Ep 220] EvalReturn=285.3  Success=20%  Eps=0.30\n",
      "[Ep 240] EvalReturn=293.4  Success=0%  Eps=0.24\n",
      "[Ep 260] EvalReturn=362.6  Success=20%  Eps=0.18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = TrainConfig(\n",
    "    episodes=400,      \n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=300,\n",
    "    update_freq=5,\n",
    "    eval_every=20,\n",
    "    eval_episodes=10,\n",
    "    max_steps=500,\n",
    ")\n",
    "\n",
    "state_dim = 4\n",
    "n_actions = 2\n",
    "\n",
    "# 1) Árbol de Decisión\n",
    "tree_agent = DecisionTreeQ(n_actions=n_actions, state_dim=state_dim, max_depth=30, min_samples_leaf=2)\n",
    "metrics_tree = train_mc_agent(tree_agent, cfg)\n",
    "\n",
    "# 2) Random Forest\n",
    "rf_agent = RandomForestQ(n_actions=n_actions, state_dim=state_dim, n_estimators=120, max_depth=30, n_jobs=-1)\n",
    "metrics_rf = train_mc_agent(rf_agent, cfg)\n",
    "\n",
    "# 3) Red Neuronal\n",
    "nn_agent = NeuralNetQ(n_actions=n_actions, state_dim=state_dim, hidden=[128,128], lr=1e-3)\n",
    "metrics_nn = train_mc_agent(nn_agent, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294cd98d",
   "metadata": {},
   "source": [
    "\n",
    "### Resultados: Curvas de Desempeño\n",
    "Mostramos la evolución del retorno por episodio y la evaluación periódica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5daf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth(x, k=11):\n",
    "    if len(x) < k:\n",
    "        return np.array(x)\n",
    "    w = np.ones(k)/k\n",
    "    return np.convolve(x, w, mode='same')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(smooth(metrics_tree[\"episode_return\"]), label=\"Train Return - Tree\")\n",
    "plt.plot(smooth(metrics_rf[\"episode_return\"]), label=\"Train Return - RF\")\n",
    "plt.plot(smooth(metrics_nn[\"episode_return\"]), label=\"Train Return - NN\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Retorno (train)\"); plt.title(\"Evolución del Retorno por Episodio\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluación\n",
    "def plot_eval(metrics, label_prefix):\n",
    "    xs = np.arange(1, len(metrics[\"eval_return\"])+1) * cfg.eval_every\n",
    "    plt.plot(xs, metrics[\"eval_return\"], label=f\"{label_prefix} EvalReturn\")\n",
    "    plt.scatter(xs, metrics[\"eval_return\"], s=10)\n",
    "    plt.twinx()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(1, len(metrics_tree[\"eval_return\"])+1) * cfg.eval_every, metrics_tree[\"eval_return\"], label=\"Tree EvalReturn\")\n",
    "plt.plot(np.arange(1, len(metrics_rf[\"eval_return\"])+1) * cfg.eval_every, metrics_rf[\"eval_return\"], label=\"RF EvalReturn\")\n",
    "plt.plot(np.arange(1, len(metrics_nn[\"eval_return\"])+1) * cfg.eval_every, metrics_nn[\"eval_return\"], label=\"NN EvalReturn\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Retorno (eval)\"); plt.title(\"Evaluación periódica\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(1, len(metrics_tree[\"eval_success\"])+1) * cfg.eval_every, np.array(metrics_tree[\"eval_success\"])*100, label=\"Tree Success %\")\n",
    "plt.plot(np.arange(1, len(metrics_rf[\"eval_success\"])+1) * cfg.eval_every, np.array(metrics_rf[\"eval_success\"])*100, label=\"RF Success %\")\n",
    "plt.plot(np.arange(1, len(metrics_nn[\"eval_success\"])+1) * cfg.eval_every, np.array(metrics_nn[\"eval_success\"])*100, label=\"NN Success %\")\n",
    "plt.xlabel(\"Episodio\"); plt.ylabel(\"Éxito (%)\"); plt.title(\"Tasa de episodios resueltos (500 pasos)\")\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f391d1",
   "metadata": {},
   "source": [
    "\n",
    "### Resumen Numérico\n",
    "Promedios de las últimas evaluaciones para comparar los tres métodos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def last_k_avg(arr, k=3):\n",
    "    if len(arr) == 0:\n",
    "        return float('nan')\n",
    "    return float(np.mean(arr[-k:]))\n",
    "\n",
    "summary = {\n",
    "    \"Agente\": [\"Decision Tree\", \"Random Forest\", \"Neural Net\"],\n",
    "    \"EvalReturn_promedio(ult3)\": [\n",
    "        last_k_avg(metrics_tree[\"eval_return\"]),\n",
    "        last_k_avg(metrics_rf[\"eval_return\"]),\n",
    "        last_k_avg(metrics_nn[\"eval_return\"]),\n",
    "    ],\n",
    "    \"Success%_promedio(ult3)\": [\n",
    "        last_k_avg(np.array(metrics_tree[\"eval_success\"])*100),\n",
    "        last_k_avg(np.array(metrics_rf[\"eval_success\"])*100),\n",
    "        last_k_avg(np.array(metrics_nn[\"eval_success\"])*100),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
